{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O\n",
    "> Functions to handle reading and writing datasets and model descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import json, os, warnings\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyreadstat\n",
    "\n",
    "import salk_toolkit as stk\n",
    "from salk_toolkit.utils import replace_constants, is_datetime, warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_json(fname,replace_const=True):\n",
    "    with open(fname,'r') as jf:\n",
    "        meta = json.load(jf)\n",
    "    if replace_const:\n",
    "        meta = replace_constants(meta)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Read files listed in meta['file'] or meta['files']\n",
    "def read_concatenate_files_list(meta,data_file=None,path=None):\n",
    "\n",
    "    opts = meta['read_opts'] if'read_opts' in meta else {}\n",
    "    if data_file: data_files = [{ 'file': data_file, 'opts': opts}]\n",
    "    elif 'file' in meta: data_files = [{ 'file': meta['file'], 'opts': opts }]\n",
    "    elif 'files' in meta: data_files = meta['files'] \n",
    "    else: raise Exception(\"No files provided\")\n",
    "    \n",
    "    data_files = [  {'opts': opts, **f } if isinstance(f,dict) else\n",
    "                    {'opts': opts, 'file': f } for f in data_files ]\n",
    "    \n",
    "    cat_dtypes = {}\n",
    "    raw_dfs, metas = [], []\n",
    "    for fi, fd in enumerate(data_files):\n",
    "        \n",
    "        data_file, opts = fd['file'], fd['opts']\n",
    "        if path: data_file = os.path.join(os.path.dirname(path),data_file)\n",
    "        \n",
    "        if data_file[-4:] == 'json' or data_file[-7:] == 'parquet': # Allow loading metafiles or annotated data\n",
    "            if data_file[-4:] == 'json': warn(f\"Processing {data_file}\") # Print this to separate warnings for input jsons from main \n",
    "            raw_data, meta = read_annotated_data(data_file, infer=False)\n",
    "            if meta is not None: metas.append(meta)\n",
    "        elif data_file[-3:] in ['csv', '.gz']:\n",
    "            raw_data = pd.read_csv(data_file, low_memory=False, **opts)\n",
    "        elif data_file[-3:] in ['sav','dta']:\n",
    "            read_fn = getattr(pyreadstat,'read_'+data_file[-3:])\n",
    "            with warnings.catch_warnings(): # While pyreadstat has not been updated to pandas 2.2 standards\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                raw_data, _ = read_fn(data_file, **{ 'apply_value_formats':True, 'dates_as_pandas_datetime':True },**opts)\n",
    "        elif data_file[-4:] in ['.xls', 'xlsx', 'xlsm', 'xlsb', '.odf', '.ods', '.odt']:\n",
    "            raw_data = pd.read_excel(data_file, **opts)\n",
    "        else:\n",
    "            raise Exception(f\"Not a known file format for {data_file}\")\n",
    "        \n",
    "        # If data is multi-indexed, flatten the index\n",
    "        if isinstance(raw_data.columns,pd.MultiIndex): raw_data.columns = [\" | \".join(tpl) for tpl in raw_data.columns]\n",
    "        \n",
    "        # Add extra columns to raw data that contain info about the file. Always includes column 'file' with filename and file_ind with index\n",
    "        # Can be used to add survey_date or other useful metainfo\n",
    "        if len(data_files)>1: raw_data['file_ind'] = fi\n",
    "        for k,v in fd.items():\n",
    "            if k in ['opts']: continue\n",
    "            if len(data_files)<=1 and k in ['file']: continue\n",
    "            raw_data[k] = v\n",
    "\n",
    "        # Strip all categorical dtypes\n",
    "        if len(data_files) > 1: # No point if only one file\n",
    "            for c in raw_data.columns:\n",
    "                if raw_data[c].dtype.name == 'category':\n",
    "                    if c not in cat_dtypes or len(cat_dtypes[c].categories)<=len(raw_data[c].dtype.categories):\n",
    "                        cat_dtypes[c] = raw_data[c].dtype\n",
    "                    raw_data[c] = raw_data[c].astype('object')\n",
    "            \n",
    "        raw_dfs.append(raw_data)\n",
    "\n",
    "    fdf = pd.concat(raw_dfs)\n",
    "\n",
    "    # Restore categoricals\n",
    "    if len(cat_dtypes)>0:\n",
    "        for c, dtype in cat_dtypes.items():\n",
    "            if not set(fdf[c].dropna().unique()) <= set(dtype.categories): # If the categories are the same, restore the dtype\n",
    "                #print(set(fdf[c].dropna().unique()), set(dtype.categories))\n",
    "                warn(f\"Categories for {c} are different between files, not restoring dtype\")\n",
    "                dtype=pd.Categorical([],list(fdf[c].dropna().unique())).dtype\n",
    "            fdf[c] = pd.Categorical(fdf[c],dtype=dtype)\n",
    "\n",
    "    return fdf, (metas[-1] if metas else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Default usage with mature metafile: process_annotated_data(<metafile name>)\n",
    "# When figuring out the metafile, it can also be run as: process_annotated_data(meta=<dict>, data_file=<>)\n",
    "def process_annotated_data(meta_fname=None, meta=None, data_file=None, raw_data=None, return_meta=False, only_fix_categories=False, return_raw=False, virtual_pass=False):\n",
    "    # Read metafile\n",
    "    if meta_fname is not None:\n",
    "        meta = read_json(meta_fname,replace_const=False)\n",
    "    \n",
    "    # Setup constants with a simple replacement mechanic\n",
    "    constants = meta['constants'] if 'constants' in meta else {}\n",
    "    meta = replace_constants(meta)\n",
    "    \n",
    "    # Read datafile(s)\n",
    "    if raw_data is None:\n",
    "        raw_data, inp_meta = read_concatenate_files_list(meta,data_file,path=meta_fname)\n",
    "        if inp_meta is not None: warn(f\"Processing main meta file\") # Print this to separate warnings for input jsons from main \n",
    "\n",
    "    if return_raw: return (raw_data, meta) if return_meta else raw_data\n",
    "    \n",
    "    globs = {'pd':pd, 'np':np, 'stk':stk, 'df':raw_data, **constants }\n",
    "    \n",
    "    pp_key = 'preprocessing' if not virtual_pass else 'virtual_preprocessing'\n",
    "    if pp_key in meta and not only_fix_categories:\n",
    "        exec(meta[pp_key],globs)\n",
    "        raw_data = globs['df']\n",
    "    \n",
    "    ndf = pd.DataFrame() if not virtual_pass else raw_data # In vitrual pass, start with the raw_data as it is already processed by normal steps\n",
    "    all_cns = dict()\n",
    "    for group in meta['structure']:\n",
    "        if group.get('virtual',False) != virtual_pass: continue\n",
    "        if group['name'] in all_cns:\n",
    "            raise Exception(f\"Group name {group['name']} duplicates a column name in group {all_cns[cn]}\") \n",
    "        all_cns[group['name']] = group['name']\n",
    "        g_cols = []\n",
    "        for tpl in group['columns']:\n",
    "            if type(tpl)==list:\n",
    "                cn = tpl[0] # column name\n",
    "                sn = tpl[1] if len(tpl)>1 and type(tpl[1])==str else cn # source column\n",
    "                cd = tpl[2] if len(tpl)==3 else tpl[1] if len(tpl)==2 and type(tpl[1])==dict else {} # metadata\n",
    "            else:\n",
    "                cn = sn = tpl\n",
    "                cd = {}\n",
    "\n",
    "            if 'scale' in group: cd = {**group['scale'],**cd}\n",
    "\n",
    "            # Col prefix is used to avoid name clashes when different groups naturally share same column names\n",
    "            if 'col_prefix' in cd: cn = cd['col_prefix']+cn\n",
    "            \n",
    "            # Detect duplicate columns in meta - including among those missing or generated\n",
    "            if cn in all_cns: \n",
    "                raise Exception(f\"Duplicate column name found: '{cn}' in {all_cns[cn]} and {group['name']}\")\n",
    "            all_cns[cn] = group['name']\n",
    "                \n",
    "            if only_fix_categories: sn = cn\n",
    "            g_cols.append(cn)\n",
    "            \n",
    "            if sn not in raw_data:\n",
    "                if not cd.get('generated') and not group.get('virtual'): # bypass warning for columns marked as being generated later\n",
    "                    warn(f\"Column {sn} not found\")\n",
    "                continue\n",
    "            \n",
    "            if raw_data[sn].isna().all():\n",
    "                warn(f\"Column {sn} is empty and thus ignored\")\n",
    "                continue\n",
    "                \n",
    "            s = raw_data[sn]\n",
    "            \n",
    "            if not only_fix_categories:\n",
    "                if s.dtype.name=='category': s = s.astype('object') # This makes it easier to use common ops like replace and fillna\n",
    "                if 'translate' in cd: \n",
    "                    s = s.astype('str').replace(cd['translate']).replace('nan',None).replace('None',None)\n",
    "                if 'transform' in cd: s = eval(cd['transform'],{ 's':s, 'df':raw_data, 'ndf':ndf, 'pd':pd, 'np':np, 'stk':stk , **constants })\n",
    "                if 'translate_after' in cd: \n",
    "                    s = pd.Series(s).astype('str').replace(cd['translate_after']).replace('nan',None).replace('None',None)\n",
    "                \n",
    "                if cd.get('datetime'): s = pd.to_datetime(s,errors='coerce')\n",
    "                elif cd.get('continuous'): s = pd.to_numeric(s,errors='coerce')\n",
    "\n",
    "            s = pd.Series(s,name=cn) # In case transformation removes the name or renames it\n",
    "\n",
    "            if 'categories' in cd: \n",
    "                na_sum = s.isna().sum()\n",
    "                \n",
    "                if cd['categories'] == 'infer':\n",
    "                    if s.dtype.name=='category': cd['categories'] = list(s.dtype.categories) # Categories come from data file\n",
    "                    elif 'translate' in cd and 'transform' not in cd and set(cd['translate'].values()) >= set(s.unique()): # Infer order from translation dict\n",
    "                        cd['categories'] = pd.unique(np.array(list(cd['translate'].values())).astype('str')).tolist()\n",
    "                        s = s.astype('str')\n",
    "                    else: # Just use lexicographic ordering\n",
    "                        if cd.get('ordered',False) and not pd.api.types.is_numeric_dtype(s):\n",
    "                            warn(f\"Ordered category {cn} had category: infer. This only works correctly if you want lexicographic ordering!\")\n",
    "                        if not pd.api.types.is_numeric_dtype(s): s = s.astype('str') # convert all to string to avoid type issues in sorting for mixed columns\n",
    "                        cd['categories'] = [ str(c) for c in np.sort(s.unique()) if pd.notna(c) ] # Also propagates it into meta (unless shared scale)\n",
    "                        s = s.astype('str')\n",
    "                    \n",
    "                cats = cd['categories']\n",
    "                s_rep = s.dropna().iloc[0] # Find a non-na element\n",
    "                if isinstance(s_rep,list) or isinstance(s_rep,np.ndarray): ns = s #  Just leave a list of strings\n",
    "                else: ns = pd.Series(pd.Categorical(s.astype('str'), # Convert to strings, even if numeric/boolean\n",
    "                                                    categories=cats,ordered=cd['ordered'] if 'ordered' in cd else False), name=cn, index=raw_data.index)\n",
    "                # Check if the category list provided was comprehensive\n",
    "                new_nas = ns.isna().sum() - na_sum\n",
    "                \n",
    "                if new_nas > 0: \n",
    "                    unlisted_cats = set(s.dropna().unique())-set(cats)\n",
    "                    warn(f\"Column {cn} {f'({sn}) ' if cn != sn else ''} had unknown categories {unlisted_cats} for { new_nas/len(ns) :.1%} entries\")\n",
    "                    \n",
    "                s = ns\n",
    "            \n",
    "            # Update ndf in real-time so it would be usable in transforms for next columns\n",
    "            if s.name in ndf.columns: ndf = ndf.drop(columns=s.name) # Overwrite existing instead of duplicates. Esp. important for virtual cols\n",
    "            ndf = pd.concat([ndf,s],axis=1)\n",
    "\n",
    "        if 'subgroup_transform' in group:\n",
    "            subgroups = group.get('subgroups',[g_cols])\n",
    "            for sg in subgroups:\n",
    "                ndf[sg] = eval(group['subgroup_transform'],{ 'gdf':ndf[sg], 'df':raw_data, 'ndf':ndf, 'pd':pd, 'np':np, 'stk':stk , **constants })\n",
    "\n",
    "    pp_key = 'postprocessing' if not virtual_pass else 'virtual_postprocessing'\n",
    "    if pp_key in meta and not only_fix_categories:\n",
    "        globs['df'] = ndf\n",
    "        exec(meta[pp_key],globs)\n",
    "        ndf = globs['df']\n",
    "    \n",
    "    return (ndf, meta) if return_meta else ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Read either a json annotation and process the data, or a processed parquet with the annotation attached\n",
    "# Return_raw is here for easier debugging of metafiles and is not meant to be used in production\n",
    "def read_annotated_data(fname, infer=True, return_raw=False, return_model_meta=False):\n",
    "    _, ext = os.path.splitext(fname)\n",
    "    meta, model_meta = None, None\n",
    "    if ext == '.json':\n",
    "        data, meta =  process_annotated_data(fname, return_meta=True, return_raw=return_raw)\n",
    "    elif ext == '.parquet':\n",
    "        data, full_meta = load_parquet_with_metadata(fname)\n",
    "        if full_meta is not None: \n",
    "            meta, model_meta = full_meta.get('data'), full_meta.get('model')\n",
    "            if meta is not None and not return_raw: # Do the second, virtual pass\n",
    "                data, meta = process_annotated_data(meta=meta, raw_data=data, virtual_pass=True, return_meta=True)\n",
    "    \n",
    "    mm = (model_meta,) if return_model_meta else tuple()\n",
    "    if meta is not None or not infer:\n",
    "        return (data, meta) + mm\n",
    "    \n",
    "    warn(f\"Warning: using inferred meta for {fname}\")\n",
    "    meta = infer_meta(fname,meta_file=False)\n",
    "    return process_annotated_data(fname, meta=meta, return_meta=True) + mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Helper functions designed to be used with the annotations\n",
    "\n",
    "# Convert data_meta into a dict where each group and column maps to their metadata dict\n",
    "def extract_column_meta(data_meta):\n",
    "    res = defaultdict(lambda: {})\n",
    "    for g in data_meta['structure']:\n",
    "        base = g['scale'] if 'scale' in g else {}\n",
    "        res[g['name']] = {**base, 'columns': [base.get('col_prefix','')+(t[0] if type(t)!=str else t) for t in g['columns']] }\n",
    "        for cd in g['columns']:\n",
    "            if isinstance(cd,str): cd = [cd]\n",
    "            res[cd[0]] = {**base,**cd[-1]} if isinstance(cd[-1],dict) else base\n",
    "    return res\n",
    "\n",
    "# Convert data_meta into a dict of group_name -> [column names]\n",
    "# TODO: deprecate - info available in extract_column_meta\n",
    "def group_columns_dict(data_meta):\n",
    "    return { k: d['columns'] for k,d in extract_column_meta(data_meta).items() if 'columns' in d }\n",
    "\n",
    "    #return { g['name'] : [(t[0] if type(t)!=str else t) for t in g['columns']] for g in data_meta['structure'] }\n",
    "\n",
    "# Take a list and a dict and replace all dict keys in list with their corresponding lists in-place\n",
    "def list_aliases(lst, da):\n",
    "    return [ fv for v in lst for fv in (da[v] if isinstance(v,str) and v in da else [v]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list_aliases(['a','b','c'],{'b': ['x','y']}) == ['a', 'x', 'y', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Creates a mapping old -> new\n",
    "def get_original_column_names(dmeta):\n",
    "    res = {}\n",
    "    for g in dmeta['structure']:\n",
    "        for c in g['columns']:\n",
    "            if isinstance(c,str): res[c] = c\n",
    "            if len(c)==1: res[c[0]] = c[0]\n",
    "            elif len(c)>=2 and isinstance(c[1],str): res[c[1]] = c[0]\n",
    "    return res\n",
    "\n",
    "# Map ot backwards and nt forwards to move from one to the other\n",
    "def change_mapping(ot, nt, only_matches=False):\n",
    "    # Todo: warn about non-bijective mappings\n",
    "    matches = { v: nt[k] for k, v in ot.items() if k in nt and v!=nt[k] } # change those that are shared\n",
    "    if only_matches: return matches\n",
    "    else: \n",
    "        return { **{ v:k for k, v in ot.items() if k not in nt }, # undo those in ot not in nt\n",
    "                 **{ k:v for k, v in nt.items() if k not in ot }, # do those in nt not in ot\n",
    "                 **matches } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Change an existing dataset to correspond better to a new meta_data\n",
    "# This is intended to allow making small improvements in the meta even after a model has been run\n",
    "# It is by no means perfect, but is nevertheless a useful tool to avoid re-running long pymc models for simple column/translation changes\n",
    "def change_meta_df(df, old_dmeta, new_dmeta):\n",
    "    warn(\"This tool handles only simple cases of column name, translation and category order changes.\")\n",
    "    \n",
    "    # Ready the metafiles for parsing\n",
    "    old_dmeta = replace_constants(old_dmeta); new_dmeta = replace_constants(new_dmeta)\n",
    "    \n",
    "    # Rename columns \n",
    "    ocn, ncn = get_original_column_names(old_dmeta), get_original_column_names(new_dmeta)\n",
    "    name_changes = change_mapping(ocn,ncn,only_matches=True)\n",
    "    if name_changes != {}: print(f\"Renaming columns: {name_changes}\")\n",
    "    df.rename(columns=name_changes,inplace=True)\n",
    "    \n",
    "    rev_name_changes = { v: k for k,v in name_changes.items() }\n",
    "    \n",
    "    # Get metadata for each column\n",
    "    ocm = extract_column_meta(old_dmeta)\n",
    "    ncm = extract_column_meta(new_dmeta)\n",
    "    \n",
    "    for c in ncm.keys():\n",
    "        if c not in df.columns: continue # probably group\n",
    "        if c not in ocm.keys(): continue # new column\n",
    "        \n",
    "        ncd, ocd = ncm[c], ocm[rev_name_changes[c] if c in rev_name_changes else c]\n",
    "        \n",
    "        # Warn about transformations and don't touch columns where those change\n",
    "        if ocd.get('transform') != ncd.get('transform'):\n",
    "            warn(f\"Column {c} has a different transformation. Leaving it unchanged\")\n",
    "            continue\n",
    "        \n",
    "        # Handle translation changes\n",
    "        ot, nt = ocd.get('translate',{}), ncd.get('translate',{})\n",
    "        remap = change_mapping(ot,nt)\n",
    "        if remap != {}: print(f\"Remapping {c} with {remap}\")\n",
    "        df[c].replace(remap,inplace=True)\n",
    "        \n",
    "        # Reorder categories and/or change ordered status\n",
    "        if ocd.get('categories') != ncd.get('categories') or ocd.get('ordered') != ncd.get('ordered'):\n",
    "            cats = ncd.get('categories')\n",
    "            if isinstance(cats,list):\n",
    "                print(f\"Changing {c} to Cat({cats},ordered={ncd.get('ordered')}\")\n",
    "                df[c] = pd.Categorical(df[c],categories=cats,ordered=ncd.get('ordered'))\n",
    "    \n",
    "    # column order changes\n",
    "    gcdict = group_columns_dict(new_dmeta)\n",
    "    \n",
    "    cols = ['draw','obs_idx'] + [ c for g in new_dmeta['structure'] for c in gcdict[g['name']]]\n",
    "    cols = [ c for c in cols if c in df.columns ]\n",
    "    \n",
    "    return df[cols]\n",
    "\n",
    "def change_parquet_meta(orig_file,data_metafile,new_file):\n",
    "    df, meta = load_parquet_with_metadata(orig_file)\n",
    "    \n",
    "    new_data_meta = read_json(data_metafile, replace_const=True)\n",
    "    df = change_meta_df(df,meta['data'],new_data_meta)\n",
    "    \n",
    "    meta['old_data'] = meta['data']\n",
    "    meta['data'] = new_data_meta\n",
    "    save_parquet_with_metadata(df,meta,new_file)\n",
    "    \n",
    "    return df, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def is_categorical(col):\n",
    "    return col.dtype.name in ['object', 'str', 'category'] and not is_datetime(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "max_cats = 50\n",
    "\n",
    "# Create a very basic metafile for a dataset based on it's contents\n",
    "# This is not meant to be directly used, rather to speed up the annotation process\n",
    "def infer_meta(data_file=None, meta_file=True, read_opts={}, df=None, translate_fn=None, translation_blacklist=[]):\n",
    "    meta = { 'constants': {}, 'read_opts': read_opts }\n",
    "    \n",
    "    # Read datafile\n",
    "    col_labels = {}\n",
    "    if data_file is not None:\n",
    "        path, fname = os.path.split(data_file)\n",
    "        meta['file'] = fname\n",
    "        if data_file[-3:] in ['csv', '.gz']:\n",
    "            df = pd.read_csv(data_file, low_memory=False, **read_opts)\n",
    "        elif data_file[-3:] in ['sav','dta']:\n",
    "            read_fn = getattr(pyreadstat,'read_'+data_file[-3:])\n",
    "            df, sav_meta = read_fn(data_file, **{ 'apply_value_formats':True, 'dates_as_pandas_datetime':True },**read_opts)\n",
    "            col_labels = dict(zip(sav_meta.column_names, sav_meta.column_labels)) # Make this data easy to access by putting it in meta as constant\n",
    "            if translate_fn: col_labels = { k:translate_fn(v) for k,v in col_labels.items() }\n",
    "        elif data_file[-7:] == 'parquet':\n",
    "            df = pd.read_parquet(data_file, **read_opts)\n",
    "        elif data_file[-4:] in ['.xls', 'xlsx', 'xlsm', 'xlsb', '.odf', '.ods', '.odt']:\n",
    "            df = pd.read_excel(data_file, **read_opts)\n",
    "        else:\n",
    "            raise Exception(f\"Not a known file format {data_file}\")\n",
    "            \n",
    "    # If data is multi-indexed, flatten the index\n",
    "    if isinstance(df.columns,pd.MultiIndex): df.columns = [\" | \".join(tpl) for tpl in df.columns]\n",
    "\n",
    "    cats, grps = {}, defaultdict(lambda: list())\n",
    "    \n",
    "    main_grp = { 'name': 'main', 'columns':[] }\n",
    "    meta['structure'] = [main_grp]\n",
    "    \n",
    "    # Remove empty columns\n",
    "    cols = [ c for c in df.columns if df[c].notna().any() ]\n",
    "    \n",
    "    # Determine category lists for all categories\n",
    "    for cn in cols:\n",
    "        if not is_categorical(df[cn]): continue\n",
    "        cats[cn] = sorted(list(df[cn].dropna().unique())) if df[cn].dtype.name != 'category' else list(df[cn].dtype.categories)\n",
    "        \n",
    "        for cs in grps:\n",
    "            #if cn.startswith('Q2_'): print(len(set(cats[cn]) & cs)/len(cs),set(cats[cn]),cs)\n",
    "            if len(set(cats[cn]) & cs)/len(cs) > 0.75: # match to group if most of the values match\n",
    "                lst = grps[cs]\n",
    "                del grps[cs]\n",
    "                grps[frozenset(cs | set(cats[cn]))] = lst + [cn]\n",
    "                break\n",
    "        else:\n",
    "            grps[frozenset(cats[cn])].append(cn)\n",
    "        \n",
    "    # Fn to create the meta for a categorical column\n",
    "    def cat_meta(cn):\n",
    "        m = { 'categories': cats[cn] if len(cats[cn])<=max_cats else 'infer' }\n",
    "        if cn in df.columns and df[cn].dtype.name=='category' and df[cn].dtype.ordered: m['ordered'] = True\n",
    "        if translate_fn is not None and cn not in translation_blacklist and len(cats[cn])<=max_cats:\n",
    "            tdict = { c: translate_fn(c) for c in m['categories'] }\n",
    "            m['categories'] = 'infer' #[ tdict[c] for c in m['categories'] ]\n",
    "            m['translate'] = tdict\n",
    "        return m\n",
    "        \n",
    "    \n",
    "    # Create groups from values that share a category\n",
    "    handled_cols = set()\n",
    "    for k,g_cols in grps.items():\n",
    "        if len(g_cols)<2: continue\n",
    "        \n",
    "        # Set up the columns part\n",
    "        m_cols = []\n",
    "        for cn in g_cols:\n",
    "            ce = [cn,{'label': col_labels[cn]}] if cn in col_labels else [cn]\n",
    "            if translate_fn is not None: ce = [translate_fn(cn)]+ ce\n",
    "            if len(ce) == 1: ce = ce[0]\n",
    "            m_cols.append(ce)\n",
    "        \n",
    "        cats[str(k)] = list(k) # so cat_meta would use the full list\n",
    "        grp = { 'name': ';'.join(k), 'scale': cat_meta(str(k)), 'columns': m_cols }\n",
    "        \n",
    "        meta['structure'].append(grp)\n",
    "        handled_cols.update(g_cols)\n",
    "        \n",
    "    # Put the rest of variables into main category\n",
    "    main_cols = [ c for c in cols if c not in handled_cols ]\n",
    "    for cn in main_cols:\n",
    "        if cn in cats: cdesc = cat_meta(cn)\n",
    "        else: \n",
    "            if is_datetime(df[cn]): cdesc = {'datetime':True}\n",
    "            else: cdesc = {'continuous':True}\n",
    "        if cn in col_labels: cdesc['label'] = col_labels[cn]\n",
    "        main_grp['columns'].append([cn,cdesc] if translate_fn is None else [translate_fn(cn),cn,cdesc])\n",
    "        \n",
    "    #print(json.dumps(meta,indent=2,ensure_ascii=False))\n",
    "    \n",
    "    # Write file to disk\n",
    "    if data_file is not None and meta_file:\n",
    "        if meta_file is True: meta_file = os.path.join(path, os.path.splitext(fname)[0]+'_meta.json')\n",
    "        if not os.path.exists(meta_file):\n",
    "            print(f\"Writing {meta_file} to disk\")\n",
    "            with open(meta_file,'w',encoding='utf8') as jf:\n",
    "                json.dump(meta,jf,indent=2,ensure_ascii=False)\n",
    "        else:\n",
    "            print(f\"{meta_file} already exists, skipping write\")\n",
    "\n",
    "    return meta\n",
    "\n",
    "# Small convenience function to have a meta available for any dataset\n",
    "def data_with_inferred_meta(data_file, **kwargs):\n",
    "    meta = infer_meta(data_file,meta_file=False, **kwargs)\n",
    "    return process_annotated_data(meta=meta, data_file=data_file, return_meta=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Check if inferred meta is usable by process_annotated_data\n",
    "df, meta = data_with_inferred_meta('../data/salk25.sav',translate_fn=lambda t: f'[[{t}]]')\n",
    "df, meta = data_with_inferred_meta('../data/master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_and_process_data(desc, return_meta=False, constants={}, skip_postprocessing=False):\n",
    "\n",
    "    df, meta = read_concatenate_files_list(desc)\n",
    "\n",
    "    if meta is None and return_meta:\n",
    "        raise Exception(\"No meta found on any of the files\")\n",
    "    \n",
    "    # Perform transformation and filtering\n",
    "    globs = {'pd':pd, 'np':np, 'stk':stk, 'df':df, **constants}\n",
    "    if 'preprocessing' in desc:  exec(desc['preprocessing'], globs)\n",
    "    if 'filter' in desc: globs['df'] = globs['df'][eval(desc['filter'], globs)]\n",
    "    if 'postprocessing' in desc and not skip_postprocessing: exec(desc['postprocessing'],globs)\n",
    "    df = globs['df']\n",
    "    \n",
    "    return (df, meta) if return_meta else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf_meta = {\n",
    "    'file': '../data/master_meta.json',\n",
    "    'preprocessing': \"df.age_group.replace({'16-24':'18-24'}, inplace=True)\",\n",
    "    'filter': '(df.citizen) & (df.age>=18) & (df.wave<5)',\n",
    "}\n",
    "\n",
    "df = read_and_process_data(dataf_meta)\n",
    "assert len(df) == 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_population_h5(fname,pdf):\n",
    "    hdf = pd.HDFStore(fname,complevel=9, complib='zlib')\n",
    "    hdf.put('population',pdf,format='table')\n",
    "    hdf.close()\n",
    "    \n",
    "def load_population_h5(fname):\n",
    "    hdf =  pd.HDFStore(fname, mode='r')\n",
    "    res = hdf['population'].copy()\n",
    "    hdf.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_sample_h5(fname,trace,COORDS = None, filter_df = None):\n",
    "    odims = [d for d in trace.predictions.dims if d not in ['chain','draw','obs_idx']]\n",
    "    \n",
    "    if COORDS is None: # Recover them from trace (requires posterior be saved in same trace)\n",
    "        inds = trace.posterior.indexes\n",
    "        coords = { t: list(inds[t]) for t in inds if t not in ['chain','draw'] and '_dim_' not in t}\n",
    "        COORDS = { 'immutable': coords, 'mutable': ['obs_idx'] }\n",
    "\n",
    "    if filter_df is None: # Recover filter dimensions and data from trace (works only for GLMs)\n",
    "        rmdims = odims + list({'time','unit','combined_inputs'} & set(trace.predictions_constant_data.dims))\n",
    "        df = trace.predictions_constant_data.drop_dims(rmdims).to_dataframe()#.set_index(demographics_order).indexb\n",
    "        df.columns = [ s.removesuffix('_id') for s in df.columns]\n",
    "        df.drop(columns=[c for c in df.columns if c[:4]=='obs_'],inplace=True)\n",
    "\n",
    "        for d in df.columns:\n",
    "            if d in COORDS['immutable']:\n",
    "                fs = COORDS['immutable'][d]\n",
    "                df[d] = pd.Categorical(df[d].replace(dict(enumerate(fs))),fs)\n",
    "                if d in orders: df[d] = pd.Categorical(df[d],orders[d],ordered=True)\n",
    "        filter_df = df\n",
    "\n",
    "    chains, draws = trace.predictions.dims['chain'], trace.predictions.dims['draw']\n",
    "    dinds = np.array(list(it.product( range(chains), range(draws), list(filter_df.index)))).reshape( (-1, 3) )\n",
    "\n",
    "    res_dfs = { 'filter': filter_df }\n",
    "    for odim in odims:\n",
    "        response_cols = list(np.array(trace.predictions[odim]))\n",
    "        xdf = pd.DataFrame(np.concatenate( (\n",
    "            dinds,\n",
    "            np.array(trace.predictions['y_'+odim]).reshape( ( -1,len(response_cols) ) )\n",
    "            ), axis=-1), columns = ['chain', 'draw', 'obs_idx'] + response_cols)\n",
    "        res_dfs[odim] = postprocess_rdf(xdf,odim)\n",
    "        \n",
    "    # Save dfs as hdf5\n",
    "    hdf = pd.HDFStore(fname,complevel=9, complib='zlib')\n",
    "    for k,vdf in res_dfs.items():\n",
    "        hdf.put(k,vdf,format='table')\n",
    "    hdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Small debug tool to help find where jsons become non-serializable\n",
    "def find_type_in_dict(d,dtype,path=''):\n",
    "    print(d,path)\n",
    "    if isinstance(d,dict):\n",
    "        for k,v in d.items():\n",
    "            find_type_in_dict(v,dtype,path+f'{k}:')\n",
    "    if isinstance(d,list):\n",
    "        for i,v in enumerate(d):\n",
    "            find_type_in_dict(v,dtype,path+f'[{i}]')\n",
    "    elif isinstance(d,dtype):\n",
    "        print(\"RES\")\n",
    "        raise Exception(f\"Value {d} of type {dtype} found at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# These two very helpful functions are borrowed from https://towardsdatascience.com/saving-metadata-with-dataframes-71f51f558d8e\n",
    "\n",
    "custom_meta_key = 'salk-toolkit-meta'\n",
    "\n",
    "def save_parquet_with_metadata(df, meta, file_name):\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    #find_type_in_dict(meta,np.int64)\n",
    "    \n",
    "    custom_meta_json = json.dumps(meta)\n",
    "    existing_meta = table.schema.metadata\n",
    "    combined_meta = {\n",
    "        custom_meta_key.encode() : custom_meta_json.encode(),\n",
    "        **existing_meta\n",
    "    }\n",
    "    table = table.replace_schema_metadata(combined_meta)\n",
    "    \n",
    "    pq.write_table(table, file_name, compression='GZIP')\n",
    "    \n",
    "# Just load the metadata from the parquet file\n",
    "def load_parquet_metadata(file_name):\n",
    "    schema = pq.read_schema(file_name)\n",
    "    if custom_meta_key.encode() in schema.metadata:\n",
    "        restored_meta_json = schema.metadata[custom_meta_key.encode()]\n",
    "        restored_meta = json.loads(restored_meta_json)\n",
    "    else: restored_meta = None\n",
    "    return restored_meta\n",
    "    \n",
    "# Load parquet with metadata\n",
    "def load_parquet_with_metadata(file_name,lazy=False,**kwargs):\n",
    "    if lazy: # Load it as a polars lazy dataframe\n",
    "        meta = load_parquet_metadata(file_name)\n",
    "        pl.scan_parquet(file_name,**kwargs)\n",
    "        return ldf, meta\n",
    "    \n",
    "    # Read it as a normal pandas dataframe\n",
    "    restored_table = pq.read_table(file_name,**kwargs)\n",
    "    restored_df = restored_table.to_pandas()\n",
    "    if custom_meta_key.encode() in restored_table.schema.metadata:\n",
    "        restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "        restored_meta = json.loads(restored_meta_json)\n",
    "    else: restored_meta = None\n",
    "\n",
    "    return restored_df, restored_meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving and loading parquet with metadata\n",
    "df = pd.DataFrame([[1,2],[3,4]],columns=['x','y'])\n",
    "meta = { 'test': [1,{'x':2},[3]] }\n",
    "\n",
    "save_parquet_with_metadata(df,meta,'test.parquet')\n",
    "ndf, nmeta = load_parquet_with_metadata('test.parquet')\n",
    "\n",
    "assert nmeta == meta\n",
    "assert ndf.equals(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you can add tests\n",
    "#assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
